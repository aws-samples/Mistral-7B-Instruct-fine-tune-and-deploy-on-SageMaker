{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603f53ab-ed44-4950-aac6-ba20b77fa16a",
   "metadata": {},
   "source": [
    "# Deploy Finetuned Mistral on Amazon SageMaker\n",
    "\n",
    "Mistral 7B is the open LLM from Mistral AI.\n",
    "\n",
    "This sample is modified from this documentation \n",
    "https://docs.djl.ai/docs/demos/aws/sagemaker/large-model-inference/sample-llm/vllm_deploy_mistral_7b.html\n",
    "\n",
    "You only need to provide the new model location to deploy the Mistral fine tune version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc64a47",
   "metadata": {},
   "source": [
    "This notebook has been tested on Amazon SageMaker Notebook Instances with single GPU on ml.g5.2xlarge\n",
    "\n",
    "The deployment has been tested on Amazon SageMaker real time inference endpoint with single GPU on ml.g5.2xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d91ea-8c0c-4157-a5fc-c16a4491f896",
   "metadata": {},
   "source": [
    "## Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "756f5440-6f6a-4329-8f03-b545b1753f9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: update when container is added to sagemaker sdk\n",
    "!pip install sagemaker huggingface_hub jinja2 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1721d88-85c2-4eec-92c6-ba1cdb28edf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::70768*******:role/service-role/AmazonSageMaker-ExecutionRole-20191024T163188\n",
      "sagemaker session region: us-east-1\n",
      "sagemaker version: 2.209.0\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c5725e0-285c-4c67-b02d-8cc60b376abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "instance_type = 'ml.g5.2xlarge'  # instances type used for the deployment "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dcac4f-5f26-4797-975b-c54f0b481ac2",
   "metadata": {},
   "source": [
    "## Download the model and upload to s3\n",
    "\n",
    "We recommend to first save the model in a S3 location and provide the S3 url in the serving.properties file. This allows faster downloads times.\n",
    "\n",
    "\n",
    "<span style=\"color:red\">*If you already download the model to the notebook, please ingore this steps*</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ba4c27-49f0-4418-bf58-7749b15b9443",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b580cd9b3354b87a1d153d22daaabe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726b4f7f834a48b1b03e2c2ebc7e2985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4ec51032c84a26be9f4ee09805aa84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66d12288630458793824c828047890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d013d5d370c4c47996640ab2e3644fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1fc3e6d6a644ba88a076cc94e11a42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db32e5873d3440d7a8d4e35aaca76571",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00acbeb1136f4052bff0651ddd3a6950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/5.06G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70aa0f18215e493eaa6d61d7ff4cae15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4539a4868143b88b4af349b6797342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac43333ef74450fb07ffd76466a88be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a2df599b624ec9a49ef8bf5667aea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91574f67a0d1464983573aab55b82cc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "\n",
    "# # - This will download the model into the current directory where ever the jupyter notebook is running\n",
    "# local_model_path = Path(\".\")\n",
    "# local_model_path.mkdir(exist_ok=True)\n",
    "\n",
    "# # Only download pytorch checkpoint files\n",
    "# allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.txt\", \"*.model\", \"*.safetensors\"]\n",
    "\n",
    "# # - Leverage the snapshot library to donload the model since the model is stored in repository using LFS\n",
    "# model_download_path = snapshot_download(\n",
    "#     repo_id=model_id,\n",
    "#     cache_dir=local_model_path,\n",
    "#     allow_patterns=allow_patterns,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb749e4d-7747-4d62-87b8-c913a4eb1e9c",
   "metadata": {},
   "source": [
    "#### If you have run the notebook of Local_Finetune_Mistral.ipynb, you should have the model artifact in Mistral-Finetuned-Merged\n",
    "\n",
    "#### If you have run the notebook of Finetune_Mistral_7B_on_Amazon_SageMaker.ipynb, you should have the model artifact in ./results/training_job, downloaded from SageMaker Training job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60b4006c-589b-4dc1-83cb-084ca6e27d88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_download_path = \"Mistral-Finetuned-Merged\"\n",
    "#model_download_path = \"./results/training_job\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b037369-a5a0-4618-b690-723d8ff6549b",
   "metadata": {},
   "source": [
    "Define where the model should upload in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "910b29d3-24f8-4073-a1f5-788404bfe635",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained model will be uploaded to ---- > s3://sagemaker-us-east-1-70768*******/mistralai/Mistral-7B-Instruct-v0.1/lmi/\n"
     ]
    }
   ],
   "source": [
    "# define a variable to contain the s3url of the location that has the model\n",
    "s3_model_prefix = f\"{model_id}/lmi\"  # folder within bucket where model artifact will go\n",
    "pretrained_model_location = f\"s3://{sagemaker_session_bucket}/{s3_model_prefix}/\"\n",
    "print(f\"Pretrained model will be uploaded to ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d022085-82e6-4d5e-a956-1fa48eaba71a",
   "metadata": {},
   "source": [
    "We upload the model files to s3 bucket, please be patient, this will takes serveral （around 20） minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c341a860-6751-417b-8b4d-49ce6ce2940c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to --- > s3://sagemaker-us-east-1-70768*******/mistralai/Mistral-7B-Instruct-v0.1/lmi\n",
      "We will set option.model_id=s3://sagemaker-us-east-1-70768*******/mistralai/Mistral-7B-Instruct-v0.1/lmi\n"
     ]
    }
   ],
   "source": [
    "model_artifact = sess.upload_data(path=model_download_path, key_prefix=s3_model_prefix)\n",
    "print(f\"Model uploaded to --- > {model_artifact}\")\n",
    "print(f\"We will set option.model_id={model_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b92b25c-6b19-4779-a1dc-5bd09336f213",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Choose an inference image\n",
    "\n",
    "[vLLM](https://github.com/vllm-project/vllm) is a fast and easy-to-use library for LLM inference and serving. Here we demo how to use VLLM to host the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5383930-19e9-47a6-b787-cdfc04b216d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker image: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.25.0-deepspeed0.11.0-cu118\n"
     ]
    }
   ],
   "source": [
    "import sagemaker \n",
    "\n",
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )\n",
    "\n",
    "#inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\"\n",
    "print(f\"Docker image: {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b6140d-66e7-41e6-a895-7e6d668efb63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Prepare serving.properties file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79641d46-a272-4447-a626-a577e9fc91bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "code_folder = \"code_vllm\"\n",
    "#pretrained_model_location = \"TheBloke/mixtral-8x7b-v0.1-AWQ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd5c3277-edcf-4651-bc04-e5bf43018776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "code_path = Path(code_folder)\n",
    "code_path.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b3660",
   "metadata": {},
   "source": [
    "Large Model Inference Configurations\n",
    "\n",
    "https://docs.djl.ai/docs/serving/serving/docs/lmi/configurations_large_model_inference_containers.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b9bd0-6193-4912-bcbe-9f26e1d3226b",
   "metadata": {},
   "source": [
    "mistralai/Mistral-7B-Instruct-v0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "339a0dba-f1eb-41df-a068-97f9fff12f54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./code_vllm/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./{code_folder}/serving.properties\n",
    "engine = Python\n",
    "option.model_id = {{s3url}}\n",
    "option.dtype=fp16\n",
    "option.tensor_parallel_degree = 1\n",
    "option.output_formatter = json\n",
    "option.task=text-generation\n",
    "option.model_loading_timeout = 1200\n",
    "option.rolling_batch=vllm\n",
    "option.device_map=auto\n",
    "option.max_model_len=2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9931d2-14da-459a-9c24-4d3301659fac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     1\t\u001b[36mengine\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mPython\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     2\t\u001b[36moption.model_id\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33ms3://sagemaker-us-east-1-70768*******/mistralai/Mistral-7B-Instruct-v0.1/lmi/\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     3\t\u001b[36moption.dtype\u001b[39;49;00m=\u001b[33mfp16\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     4\t\u001b[36moption.tensor_parallel_degree\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m1\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     5\t\u001b[36moption.output_formatter\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33mjson\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     6\t\u001b[36moption.task\u001b[39;49;00m=\u001b[33mtext-generation\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     7\t\u001b[36moption.model_loading_timeout\u001b[39;49;00m\u001b[37m \u001b[39;49;00m=\u001b[37m \u001b[39;49;00m\u001b[33m600\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     8\t\u001b[36moption.rolling_batch\u001b[39;49;00m=\u001b[33mvllm\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "     9\t\u001b[36moption.device_map\u001b[39;49;00m=\u001b[33mauto\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "import jinja2\n",
    "jinja_env = jinja2.Environment()\n",
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(f\"{code_folder}/serving.properties\").open().read())\n",
    "Path(f\"{code_folder}/serving.properties\").open(\"w\").write(\n",
    "    template.render(s3url=pretrained_model_location)\n",
    ")\n",
    "!pygmentize {code_folder}/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd1f43e-4448-47ba-aefd-9924da9d4d18",
   "metadata": {},
   "source": [
    "### create tarball and upload to s3 location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "818ddddd-36ea-4bc6-bc46-6601e52a69eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./serving.properties\n"
     ]
    }
   ],
   "source": [
    "!rm -f model.tar.gz\n",
    "!rm -rf {code_folder}/.ipynb_checkpoints\n",
    "!tar czvf model.tar.gz -C {code_folder} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d34bebe9-682a-4561-b03e-c2bfd7dedf43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 Code or Model tar ball uploaded to --- > s3://sagemaker-us-east-1-70768*******/mistralai/Mistral-7B-Instruct-v0.1/code_vllm/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "s3_code_prefix = f\"{model_id}/{code_folder}\"\n",
    "s3_code_artifact = sess.upload_data(\"model.tar.gz\", sagemaker_session_bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6027cb0e-27b1-41ac-a61d-f142bea36f7e",
   "metadata": {},
   "source": [
    "## Create the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24a5dacf-b66c-4704-aaba-aa7fbc159f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/Mistral-7B-Instruct-v0.1\n",
      "ml.g5.2xlarge\n"
     ]
    }
   ],
   "source": [
    "## Deploy Mixtral 8x7B to Amazon SageMaker\n",
    "\n",
    "import json\n",
    "from sagemaker import Model\n",
    "from datetime import datetime\n",
    "\n",
    "# sagemaker config\n",
    "print(model_id)\n",
    "print(instance_type)\n",
    "health_check_timeout = 1200\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = Model(\n",
    "  role=role,\n",
    "  name=f\"vllm-{model_id.replace('/', '-').lower().replace('.', '')}-{timestamp}\",\n",
    "  model_data=s3_code_artifact, \n",
    "  image_uri=inference_image_uri,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa217f75-4aff-4753-8333-df4928def65c",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.48xlarge` instance type. TGI will automatically distribute and shard the model across all GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5794eb27-441f-48e5-9591-3f63b0921647",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe4457b7-bfa7-44c7-b0c8-d3f4d06c7372",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vllm-mistralai-mistral-7b-instruct-v01--2024-02-28-07-04-46-288\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = llm_model.endpoint_name\n",
    "print(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955f836-4928-4098-8bb4-7118ba0a7781",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run inference and chat with the model\n",
    "\n",
    "See below as one example of prompt and response "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f67d3b-edd0-4cb5-972d-96e72b2a47f0",
   "metadata": {},
   "source": [
    "[INST] Below is the question based on the context. Question: Given a reference text about Lollapalooza, where does it take place, who started it and what is it?. Below is the given the context Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States.\n",
    "Lollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.. Write a response that appropriately completes the request.[/INST]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1587944-6ba0-48a7-b96a-2263f19dc6ac",
   "metadata": {},
   "source": [
    "#### Dataset ground truth:\n",
    "Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21abbf85-05df-453b-8704-65d229abe84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create a Boto3 client for SageMaker Runtime\n",
    "sagemaker_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "max_tokens_to_sample = 200\n",
    " \n",
    "# Define the prompt and other parameters\n",
    "prompt = \"\"\"\n",
    "<s>[INST] Below is the question based on the context. \n",
    "Question: Given a reference text about Lollapalooza, where does it take place, who started it and what is it?. \n",
    "Below is the given the context Lollapalooza /ˌlɒləpəˈluːzə/ (Lolla) is an annual American four-day music festival held in Grant Park in Chicago. \n",
    "It originally started as a touring event in 1991, but several years later, Chicago became its permanent location. Music genres include but are not limited to alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. Lollapalooza has also featured visual arts, nonprofit organizations, and political organizations. \n",
    "The festival, held in Grant Park, hosts an estimated 400,000 people each July and sells out annually. Lollapalooza is one of the largest and most iconic music festivals in the world and one of the longest-running in the United States. Lollapalooza was conceived and created in 1991 as a farewell tour by Perry Farrell, singer of the group Jane's Addiction.. \n",
    "Write a response that appropriately completes the request.[/INST]\n",
    "\"\"\"\n",
    "\n",
    "# hyperparameters for llm\n",
    "parameters = {\n",
    "    \"max_new_tokens\": max_tokens_to_sample,\n",
    "    \"do_sample\": True,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.5,\n",
    "}\n",
    "\n",
    "contentType = 'application/json'\n",
    "\n",
    "body = json.dumps({\n",
    "    \"inputs\": prompt,\n",
    "    # specify the parameters as needed\n",
    "    \"parameters\": parameters\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f4376239-b8a7-476a-a874-ccc10a2b2303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = sagemaker_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name, Body=body, ContentType=contentType)\n",
    "\n",
    "# Process the response\n",
    "response_body = json.loads(response.get('Body').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e2508ff-1f63-4634-818c-50a0424fd132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Lollapalooze is an annual musical festival held in Grant Park in Chicago, Illinois. It was started in 1991 as a farewell tour by Perry Farrell, singe of the group Jane's Addiction. The festival includes an array of musical genres including alternative rock, heavy metal, punk rock, hip hop, and electronic dance music. The festivals welcomes an estimated 400,000 people each year and sells out annually. Some notable headliners include: the Red Hot Chili Peppers, Chance the Rapper, Metallica, and Lady Gage. Lollapalooza is one of the largest and most iconic festivals in the world and a staple of Chicago.\n"
     ]
    }
   ],
   "source": [
    "print(response_body['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a4186-0d35-44f2-9040-b87321bc9e80",
   "metadata": {},
   "source": [
    "## Clean up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20046d-6f3b-40b7-a984-414a4e5af055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm.delete_model()\n",
    "#llm.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
