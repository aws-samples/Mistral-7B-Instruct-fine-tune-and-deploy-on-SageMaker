{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82d23eb4-072b-4832-b7ff-6d9516bb1d3c",
   "metadata": {},
   "source": [
    "# Deploying Large Language Models Using the VLLM Backend for General Inference\n",
    "\n",
    "\n",
    "In this tutorial, you will employ the VLLM backend of the Large Model Inference (LMI) DLC to deploy a Hugging Face model and use boto3 to test the inference capabilities, including options for streaming and non-streaming features.\n",
    "\n",
    "Please ensure that your machine has sufficient disk space before proceeding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7741f93b-8127-4d2f-9e08-804cf5e516a2",
   "metadata": {},
   "source": [
    "## Step 1: Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9980179f-2656-4f3a-a662-57d7eefb45b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker>=2.216.0\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220210b-6db2-42c3-a36e-947f82645160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub jinja2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4237222b-a70d-4c77-b8b0-e5114308634d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9667c3-d3b8-4ad8-845f-2eecbee8443a",
   "metadata": {},
   "source": [
    "## Step 2: Start preparing model artifacts\n",
    "In LMI container, we expect some artifacts to help setting up the model\n",
    "\n",
    "- serving.properties (required): Defines the model server settings\n",
    "- model.py (optional): A python file to define the core inference logic\n",
    "- requirements.txt (optional): Any additional pip wheel need to install\n",
    "\n",
    "For the purpose of this tutorial, we will focus only on the `serving.properties` file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1fcbda-113c-4eb3-94de-1e31884c64db",
   "metadata": {},
   "source": [
    "### Download the model to local and upload to s3\n",
    "\n",
    "Please skip this step if you already possess a model on S3, whether it's a downloaded or a fine-tuned version.\n",
    "\n",
    "\n",
    "Update your model ID and Hugging Face token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd51cba-dfc8-4158-9fec-a52bee345be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id =\"****\"\n",
    "huggingface_token = \"****\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc86b0b-2380-43d8-9e45-ae78d3ae4848",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "# Download the model repository from the Hugging Face Hub\n",
    "model_directory = snapshot_download(model_id, token= huggingface_token, local_dir=f\"/home/ec2-user/SageMaker/{model_id}\", ignore_patterns=[\"*.pth\", \"original/*\"])\n",
    "print(f\"Downloaded model {model_id} to {model_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e7519e-b461-44b2-926b-07fae6710016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Upload to s3\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "S3Uploader.upload(\n",
    "        local_path=model_id,\n",
    "        desired_s3_uri=f\"s3://{sagemaker_session_bucket}/models/{model_id}\",\n",
    "        sagemaker_session=sess\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c17391-9e16-470e-aad4-d9a152f7ee31",
   "metadata": {},
   "source": [
    "### Prepare the serving.properties\n",
    "\n",
    "Update the model location to the correct S3 path. If you have fine-tuned a model stored in S3, please change the value to reflect your specific S3 bucket location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686fe4a-6901-463e-a5fa-6af9ce5a5ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jinja2\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory path\n",
    "deployment_path = \"deployment\"\n",
    "\n",
    "# Check if the directory exists. If not, create it.\n",
    "os.makedirs(deployment_path, exist_ok=True)\n",
    "\n",
    "jinja_env = jinja2.Environment()\n",
    "\n",
    "template = jinja_env.from_string(Path(\"serving.template\").open().read())\n",
    "Path(f\"{deployment_path}/serving.properties\").open(\"w\").write(\n",
    "    template.render(model_id=f\"s3://{sagemaker_session_bucket}/models/{model_id}\")\n",
    "\n",
    ")\n",
    "!pygmentize deployment/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03a19ad-fcc2-41f3-bbcf-01e745db6fe8",
   "metadata": {},
   "source": [
    "Pack your serviing.properties in a tar file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74176a52-7ecd-414c-bb95-4a07d26e2851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir mymodel\n",
    "rm -f mymodel.tar.gz\n",
    "mv deployment/serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7872a3c-fd31-4102-b154-cfea6b804fcf",
   "metadata": {},
   "source": [
    "## Step 3: Start building SageMaker endpoint\n",
    "\n",
    "Getting the container image URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2710a93c-8d0c-40c2-a87a-d62206346c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris \n",
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-deepspeed\",\n",
    "        region=sess.boto_session.region_name,\n",
    "        version=\"0.27.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68729439-b647-447b-b128-0466b8231110",
   "metadata": {},
   "source": [
    "Upload artifact on S3 and create SageMaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7886ed-8dda-460e-8442-7d2e3829dd67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import Model\n",
    "\n",
    "s3_code_prefix = f\"large-model-vllm/{model_id}_code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "\n",
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b557f038-76c8-4593-9e4e-48fc2dff5800",
   "metadata": {},
   "source": [
    "Deploy SageMaker endpoint\n",
    "- instance_type = \"ml.g5.2xlarge\": This line sets the type of machine that SageMaker will use to host the endpoint. The instance type ml.g5.2xlarge is generally suitable for demanding machine learning tasks. If you are planning to use larger token lengths in your model, you might need to choose a more powerful instance type to ensure optimal performance.\n",
    "- endpoint_name = sagemaker.utils.name_from_base(f\"lmi-model-{model_id.replace('/', '-')}\"): This line generates a unique name for the SageMaker endpoint. It uses the model ID, modifying it to replace slashes with hyphens to create a valid endpoint name. This is necessary because certain characters like slashes may not be permitted in AWS resource names.\n",
    "- model.deploy(...): This function call deploys the model to the configured SageMaker endpoint. Here are the parameters used:\n",
    "   * initial_instance_count=1: This specifies that one instance of the specified type should be used.\n",
    "   * instance_type: As defined earlier, this is the type of instance to deploy.\n",
    "   * endpoint_name: The unique name generated for the endpoint.\n",
    "   *  container_startup_health_check_timeout=1800: This sets a timeout value in seconds for the container startup  \n",
    "   health check, ensuring that the deployment does not hang indefinitely if issues occur during startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba8fdfd-c2c9-4c00-8de1-6e08b70a7be8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the instance type; update this if using larger token lengths\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(f\"lmi-model-{model_id.replace('/', '-')}\")\n",
    "print(f\"endpoint_name: {endpoint_name}\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             container_startup_health_check_timeout=1800\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f27076f-db37-4b45-a4b8-5cd47beb175d",
   "metadata": {},
   "source": [
    "## Step 4: Run inference\n",
    "\n",
    "In the example below, we demonstrate the inference process using a sample question as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923797e4-8f2e-4291-bb35-76cec20237da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question= \"tell me about Harry Potter in 100 words\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87abee78-54e6-4c4e-8348-ee6fa6aa35de",
   "metadata": {},
   "source": [
    "### Normal request\n",
    "\n",
    "To query an Amazon SageMaker model endpoint effectively, you use the invoke_endpoint API provided by the SageMaker Runtime service. This API allows you to send input data to your deployed model and receive predictions in response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6698e-7271-465f-82ca-e3346724ea84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_data = {\n",
    "    \"inputs\": f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\\n\\n\", \n",
    "    \"parameters\": {\"max_new_tokens\":1024}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b6bed-e201-490b-b4f2-cbc0107658c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# Create a SageMaker runtime client with the AWS SDK\n",
    "client = boto3.client('sagemaker-runtime')\n",
    "\n",
    "# Convert the input data to JSON string\n",
    "payload = json.dumps(input_data)\n",
    "\n",
    "# Set the content type for the endpoint, adjust if different for your model\n",
    "content_type = \"application/json\"\n",
    "\n",
    "# Invoke the SageMaker endpoint\n",
    "response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=content_type,\n",
    "        Body=payload\n",
    ")\n",
    "\n",
    "# The response is a stream of bytes. We need to read and decode it.\n",
    "result = response['Body'].read().decode('utf-8')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a41fe-3417-457a-8d06-580d3ad230a3",
   "metadata": {},
   "source": [
    "### Streaming\n",
    "\n",
    "The invoke_endpoint_with_response_stream function is an API provided by Amazon SageMaker, designed to handle streaming responses from a deployed model endpoint. \n",
    "\n",
    "The `LineIterator` is copied from https://github.com/deepjavalibrary/djl-demo/blob/master/aws/sagemaker/large-model-inference/sample-llm/utils/LineIterator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121de517-f590-4f2f-9307-a0c3a3a9f8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from utils.LineIterator import LineIterator\n",
    "\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "def get_realtime_response_stream(sagemaker_runtime, endpoint_name, payload):\n",
    "    response_stream = sagemaker_runtime.invoke_endpoint_with_response_stream(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload), \n",
    "        ContentType=\"application/json\"\n",
    "    )\n",
    "    return response_stream\n",
    "\n",
    "\n",
    "\n",
    "def print_response_stream(response_stream):\n",
    "    event_stream = response_stream.get('Body')\n",
    "    last_error_line =''\n",
    "    for line in LineIterator(event_stream):\n",
    "        try:\n",
    "            print(json.loads(last_error_line+line)[\"token\"][\"text\"], end='')\n",
    "            last_error_line =''\n",
    "        except:\n",
    "            last_error_line = line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2898fd61-2323-498c-92d8-54853ae187f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {    \n",
    "    \"inputs\":  f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\\n\\n\", \n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\":1024, \n",
    "        \"stop\":[\"<|start_header_id|>\", \"<|end_header_id|>\", \"<|eot_id|>\", \"<|reserved_special_token\"]\n",
    "    },\n",
    "    \"stream\": True ## <-- to have response stream.\n",
    "}\n",
    "response_stream = get_realtime_response_stream(smr_client, endpoint_name, payload)\n",
    "print_response_stream(response_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b766c8-c97c-4189-a682-5a9d6f66fad4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clear Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b38fd78-6ef3-4d08-9ea4-bb6b96fe3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Further reading\n",
    "\n",
    "Please visit https://github.com/deepjavalibrary/djl-demo/tree/master/aws/sagemaker/large-model-inference/sample-llm \n",
    "for more LMI usage tutorials."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9a3f28be3b9bb769"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
