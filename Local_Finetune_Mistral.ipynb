{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "603f53ab-ed44-4950-aac6-ba20b77fa16a",
   "metadata": {},
   "source": [
    "# Local Finetune Mistral 7B\n",
    "\n",
    "Mistral 7B is the open LLM from Mistral AI.\n",
    "\n",
    "This sample is modified from this tutorial \n",
    "https://adithyask.medium.com/a-beginners-guide-to-fine-tuning-mistral-7b-instruct-model-0f39647b20fe\n",
    "\n",
    "What this tutorial will, step-by-step, cover:\n",
    "\n",
    "- Setup Development Environment\n",
    "- Load and prepare the dataset\n",
    "- Fine-Tune Mistral with QLoRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81be81",
   "metadata": {},
   "source": [
    "This notebook has been tested on Amazon SageMaker Notebook Instances with single GPU on ml.g5.2xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d91ea-8c0c-4157-a5fc-c16a4491f896",
   "metadata": {},
   "source": [
    "## Setup development environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4150b595-3807-48d9-b60e-75c69638e4c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.38.1 datasets==2.17.1 peft==0.8.2 bitsandbytes==0.42.0 trl==0.7.11 --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2891d64-4920-465e-9781-c86dcdd31743",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and prepare the dataset\n",
    "\n",
    "\n",
    "### Choose a dataset\n",
    "\n",
    "For the purpose of this tutorial, we will use dolly, an open-source dataset containing 15k instruction pairs.\n",
    "\n",
    "Example record from dolly:\n",
    "```\n",
    "{\n",
    "  \"instruction\": \"Who was the first woman to have four country albums reach No. 1 on the Billboard 200?\",\n",
    "  \"context\": \"\",\n",
    "  \"response\": \"Carrie Underwood.\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c248c8fa-1026-4e5b-9182-9ee37e64e993",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size: 20\n",
      "{'instruction': 'Why mobile is bad for human', 'context': '', 'response': 'We are always engaged one phone which is not good.', 'category': 'brainstorming'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randrange\n",
    "\n",
    "# Load dataset from the hub\n",
    "#dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "#For local testing the fine tuning code, we limit the dataset to 20 samples \n",
    "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\").select(range(20))\n",
    "\n",
    "print(f\"dataset size: {len(dataset)}\")\n",
    "print(dataset[randrange(len(dataset))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd4df77-add2-461b-bf92-cb8d6a4c9afe",
   "metadata": {},
   "source": [
    "### Understand the Mistral format and prepare the prompt input\n",
    "\n",
    "The mistralai/Mistral-7B-Instruct-v0.1 is a conversational chat model meaning we can chat with it using the following prompt:\n",
    "\n",
    "\n",
    "```\n",
    "<s> [INST] User Instruction 1 [/INST] Model answer 1</s> [INST] User instruction 2 [/INST]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc630c2-3682-4d8d-a321-14f64374a0d1",
   "metadata": {},
   "source": [
    "For instruction fine-tuning, it is quite common to have two columns inside the dataset: one for the prompt & the other for the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e62a310d-d98a-460d-b00a-78e7f4f60882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "# Define the create_prompt function\n",
    "def create_prompt(sample):\n",
    "    bos_token = \"<s>\"\n",
    "    eos_token = \"</s>\"\n",
    "    \n",
    "    instruction = sample['instruction']\n",
    "    context = sample['context']\n",
    "    response = sample['response']\n",
    "\n",
    "    text_row = f\"\"\"[INST] Below is the question based on the context. Question: {instruction}. Below is the given the context {context}. Write a response that appropriately completes the request.[/INST]\"\"\"\n",
    "    answer_row = response\n",
    "\n",
    "    sample[\"prompt\"] = bos_token + text_row\n",
    "    sample[\"completion\"] = answer_row + eos_token\n",
    "\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e322854-8f7c-41b4-9f69-627cc35058a2",
   "metadata": {},
   "source": [
    "lets test our formatting function on a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6e67f6-7705-4a7f-936c-4dcb9a6f921a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f5583c0cc4343f4bd0628ef603d2797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': \"<s>[INST] Below is the question based on the context. Question: Who was John Moses Browning?. Below is the given the context John Moses Browning (January 23, 1855 – November 26, 1926) was an American firearm designer who developed many varieties of military and civilian firearms, cartridges, and gun mechanisms – many of which are still in use around the world. He made his first firearm at age 13 in his father's gun shop and was awarded the first of his 128 firearm patents on October 7, 1879, at the age of 24. He is regarded as one of the most successful firearms designers of the 19th and 20th centuries and pioneered the development of modern repeating, semi-automatic, and automatic firearms.\\n\\nBrowning influenced nearly all categories of firearms design, especially the autoloading of ammunition. He invented, or made significant improvements to, single-shot, lever-action, and pump-action rifles and shotguns. He developed the first reliable and compact autoloading pistols by inventing the telescoping bolt, then integrating the bolt and barrel shroud into what is known as the pistol slide. Browning's telescoping bolt design is now found on nearly every modern semi-automatic pistol, as well as several modern fully automatic weapons. He also developed the first gas-operated firearm, the Colt–Browning Model 1895 machine gun – a system that surpassed mechanical recoil operation to become the standard for most high-power self-loading firearm designs worldwide. He also made significant contributions to automatic cannon development.\\n\\nBrowning's most successful designs include the M1911 pistol, the water-cooled M1917, the air-cooled M1919, and heavy M2 machine guns, the M1918 Browning Automatic Rifle, and the Browning Auto-5 – the first semi-automatic shotgun. Some of these arms are still manufactured, often with only minor changes in detail and cosmetics to those assembled by Browning or his licensees. The Browning-designed M1911 and Hi-Power are some of the most copied firearms in the world.. Write a response that appropriately completes the request.[/INST]\", 'completion': \"John Moses Browning is one of the most well-known designer of modern firearms.  He started building firearms in his father's shop at the age of 13, and was awarded his first patent when he was 24.\\n\\nHe  designed the first reliable automatic pistol, and the first gas-operated firearm, as well inventing or improving single-shot, lever-action, and pump-action rifles and shotguns.\\n\\nToday, he is most well-known for the M1911 pistol, the Browning Automatic Rifle, and the Auto-5 shotgun, all of which are in still in current production in either their original design, or with minor changes.  His M1911 and Hi-Power pistols designs are some of the most reproduced firearms in the world today.</s>\"}\n"
     ]
    }
   ],
   "source": [
    "dataset_instruct_format = dataset.map(create_prompt, remove_columns=['instruction','context','response','category'])\n",
    "# print random sample\n",
    "print(dataset_instruct_format[randint(0, len(dataset_instruct_format))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae72dc8-48ee-4dd2-b067-a973baa48a86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9d1aa8e-84ea-4555-8fb0-5ce45850a48e",
   "metadata": {},
   "source": [
    "### Prepare the configuration for training the LLM\n",
    "\n",
    "Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA\n",
    "https://huggingface.co/blog/4bit-transformers-bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31f3b9c0-4b26-4984-b166-75da00a557a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "new_model = \"Mistral-qlora-7B-Instruct-v0.1\" #set the name of the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1453314-9347-44e6-9472-34305dad5342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c68c3166-10d3-48c4-b6f2-caa1b3d4c69d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "################################################################################\n",
    "# QLoRA parameters\n",
    "################################################################################\n",
    "\n",
    "# LoRA attention dimension\n",
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"bfloat16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = False\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 4\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 4\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 25\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 25\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}\n",
    "#device_map = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20be602f-8400-4290-9103-568e0776dd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "496bc29f1dbc4329965782365e1bcebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4014baf66b4149749abdd5f72a34bfba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fa7fc8d2ef40bc9cc574dc5e3e4149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24418f6c9cb047a0ac45ca822660f09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "355c8f94a315400197731f79a4d982da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e211909071648b0932d38780ed5869f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa1e790f9f9419e8691057707a95472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17932bbe7c84821892dfabeb2a6ec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330d14b156ce4c8c939e4fbba3fed4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9814134d114f86abb395c681ecfc33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88efbfa14c874bdb8f8942c269613212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "# Load the base model with QLoRA configuration\n",
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "\n",
    "base_model.config.use_cache = False\n",
    "base_model.config.pretraining_tp = 1\n",
    "\n",
    "# Load MitsralAi tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71488fc2-a707-4b1f-a12d-bb169eb6c85e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db7ea67-cb73-43bc-8d99-9981429499c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, bnb.nn.Linear4bit):\n",
    "            names = name.split(\".\")\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if \"lm_head\" in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove(\"lm_head\")\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3efac0-8989-43a9-937c-64921015b73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get lora target modules\n",
    "modules = find_all_linear_names(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69921e29-c5d8-4a93-a8e1-9e4b6006ec9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843c9954-95f5-4761-a85b-39caa2f8889c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Inference using base model only before fine tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba52c5d-93a5-4328-9d89-80d46323e54a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_prompt = create_prompt(dataset[randrange(len(dataset))])[\"prompt\"]\n",
    "\n",
    "# # import random\n",
    "# model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# base_model.eval()\n",
    "# with torch.no_grad():\n",
    "#     print(tokenizer.decode(base_model.generate(**model_input, max_new_tokens=256, pad_token_id=2)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d42fbd4-35e2-4a8a-ab82-b4b8be07bc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "# Set LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Set training parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=100, # the total number of training steps to perform\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b38140-fcfc-4786-84fd-a7c3590811c5",
   "metadata": {},
   "source": [
    "Train on completions only https://huggingface.co/docs/trl/en/sft_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1128648c-c865-4ac2-9823-4954c5915201",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['prompt'])):\n",
    "        text = f\"{example['prompt'][i]}\\n\\n ### Answer: {example['completion'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "response_template = \"### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43f1ab-e0a6-49ed-b09d-7286f09f3d3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the SFTTrainer for fine-tuning\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset_instruct_format,\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,  # You can specify the maximum sequence length here\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e873e68-01ee-4d42-b7e1-edc375b79157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start the training process\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5b6b7e-e8c1-458d-a7b3-fa5d08f703d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81708229-ed18-4a35-909c-5a8abd2dc473",
   "metadata": {},
   "source": [
    "### Finished all training steps ...  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daeb016f-0a5f-4202-8c5d-d87e6f6dd190",
   "metadata": {},
   "source": [
    "#### Merge the trained qlora into the base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5183c57d-b905-4adb-adbf-fc788861d0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=device_map\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9d4b5a-d421-4941-9606-3151d3b9d988",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fefd55-780a-4edd-a407-8d38eeb98314",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model= PeftModel.from_pretrained(base_model, new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa1c293-1daa-45a3-8531-9334163ba123",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019bb5c-3215-4bf4-9c9c-1a9e84792739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model= merged_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed33bd53-467a-4d57-8265-8ced8a7a01d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(merged_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5879d-06e4-4146-80c1-f8a026854456",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_save_dir = \"Mistral-Finetuned-Merged\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ef243d-0356-4c7f-859a-83800ce61d3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(sagemaker_save_dir, safe_serialization=True, max_shard_size=\"2GB\")\n",
    "# save tokenizer for easy inference\n",
    "tokenizer.save_pretrained(sagemaker_save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8945c6-df65-4f4f-90fe-792c5155e73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf36938-a425-43ed-aa0f-938edb4a026b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
